{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import PartialState\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "from attn_map_utils import register_cross_attention_hook\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler\n",
    "from diffusers.utils import load_image\n",
    "from hi_sam.text_segmentation import make_text_segmentation_args\n",
    "from text_diffuser.generate_mask_only import gen_mask_only\n",
    "from text_diffuser.pipeline_text_diffuser_sd15 import StableDiffusionPipeline\n",
    "from text_diffuser.t_diffusers.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "# input_image = Image.open(\"text_diffuser/assets/test01.jpeg\").convert(\"RGB\").resize((512,512))\n",
    "hf_dataset_base_url = \"https://huggingface.co/datasets/GoGiants1/TMDBEval500/resolve/main/TMDBEval500/images/\"\n",
    "input_image = load_image(hf_dataset_base_url + '3.jpg')\n",
    "guidance_scale = 7\n",
    "\n",
    "\"\"\" Change the text in the original image by coordinates and  \"\"\"\n",
    "\n",
    "sample_text=\"MLVU Project\"\n",
    "prompt = \"a tiger and a lion, talk together\"\n",
    "\n",
    "\n",
    "\n",
    "# for original_input.jpeg. 110, 500에서 가장 가까운 mask의 글자를 바꾼다.\n",
    "coordinates=[[256, 256]] \n",
    "arg_textseg = make_text_segmentation_args(\n",
    "    model_type='vit_l',\n",
    "    checkpoint_path='sam_tss_l_hiertext.pth',\n",
    "    input_size=input_image.size,\n",
    "    hier_det=False,\n",
    ")\n",
    "\n",
    "arg_maskgen = make_text_segmentation_args(\n",
    "    model_type='vit_h',\n",
    "    checkpoint_path='word_detection_totaltext.pth',\n",
    "    input_size=input_image.size,\n",
    "    hier_det=True,\n",
    ")\n",
    "\n",
    "out = gen_mask_only(input_image, sample_text=sample_text, coordinates=coordinates, arg_textseg=arg_textseg, arg_maskgen=arg_maskgen)\n",
    "\n",
    "\n",
    "img = out\n",
    "img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, binary_tss = cv2.threshold(\n",
    "    gray, 50, 255, cv2.THRESH_BINARY\n",
    ")\n",
    "\n",
    "_, binary_bbox = cv2.threshold(\n",
    "    gray, 200, 255, cv2.THRESH_BINARY\n",
    ")\n",
    "\n",
    "binary_tss_pil = Image.fromarray(binary_tss, 'L')\n",
    "binary_bbox_pil = Image.fromarray(binary_bbox, 'L')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_ckpt = \"GoGiants1/td-unet15\"\n",
    "\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    td_ckpt,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", use_safetensors=True)\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "# distributed_state = PartialState()\n",
    "# pipe.to(distributed_state.device)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# pipe.to(\"cuda\")\n",
    "\n",
    "pipe.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=[\n",
    "        \"models\",\n",
    "        \"models\",\n",
    "    ],\n",
    "    weight_name=[\n",
    "        # \"ip-adapter_sd15.bin\",\n",
    "        # \"ip-adapter_sd15.bin\",\n",
    "        \"ip-adapter-plus_sd15.safetensors\",\n",
    "        \"ip-adapter-plus_sd15.safetensors\",\n",
    "        \n",
    "    ],\n",
    ")\n",
    "pipe.set_ip_adapter_scale(0.25)\n",
    "\n",
    "# pipe.unet = register_cross_attention_hook(pipe.unet)\n",
    "\n",
    "\n",
    "\"\"\" Change the text in the original image by coordinates and  \"\"\"\n",
    "\n",
    "text_mask_image = cv2.cvtColor(np.array(out), cv2.COLOR_RGB2BGR)\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True, algorithm_type=\"sde-dpmsolver++\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    input_image=input_image,\n",
    "    text_mask_image=text_mask_image,\n",
    "    ip_adapter_image=[input_image, input_image],\n",
    "    width=512,\n",
    "    height=512,\n",
    "    guidance_scale=5,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "output.save(f\"experiments/td-15-style-transfer/{prompt}/output_cfg_{guidance_scale}.png\", \"PNG\")\n",
    "input_image.save(f\"experiments/td-15-style-transfer/{prompt}/input.png\", \"PNG\")\n",
    "text_mask_image = binary_tss_pil.save(f\"experiments/td-15-style-transfer/{prompt}/text_mask.png\", \"PNG\")\n",
    "# img = make_image_grid([input_image, output], rows=1, cols=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
