{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyclipper\n",
    "import torch\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hi_sam.models.build import model_registry\n",
    "from hi_sam.models.predictor import SamPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unclip(p, unclip_ratio=2.0):\n",
    "    poly = Polygon(p)\n",
    "    distance = poly.area * unclip_ratio / poly.length\n",
    "    offset = pyclipper.PyclipperOffset()\n",
    "    offset.AddPath(p, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "    expanded = np.array(offset.Execute(distance))\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def polygon2rbox(polygon, image_height, image_width):\n",
    "    rect = cv2.minAreaRect(polygon)\n",
    "    corners = cv2.boxPoints(rect)\n",
    "    corners = np.array(corners, dtype=\"int\")\n",
    "    pts = get_tight_rect(corners, 0, 0, image_height, image_width, 1)\n",
    "    pts = np.array(pts).reshape(-1, 2)\n",
    "    return pts\n",
    "\n",
    "\n",
    "def get_tight_rect(points, start_x, start_y, image_height, image_width, scale):\n",
    "    points = list(points)\n",
    "    ps = sorted(points, key=lambda x: x[0])\n",
    "\n",
    "    if ps[1][1] > ps[0][1]:\n",
    "        px1 = ps[0][0] * scale + start_x\n",
    "        py1 = ps[0][1] * scale + start_y\n",
    "        px4 = ps[1][0] * scale + start_x\n",
    "        py4 = ps[1][1] * scale + start_y\n",
    "    else:\n",
    "        px1 = ps[1][0] * scale + start_x\n",
    "        py1 = ps[1][1] * scale + start_y\n",
    "        px4 = ps[0][0] * scale + start_x\n",
    "        py4 = ps[0][1] * scale + start_y\n",
    "    if ps[3][1] > ps[2][1]:\n",
    "        px2 = ps[2][0] * scale + start_x\n",
    "        py2 = ps[2][1] * scale + start_y\n",
    "        px3 = ps[3][0] * scale + start_x\n",
    "        py3 = ps[3][1] * scale + start_y\n",
    "    else:\n",
    "        px2 = ps[3][0] * scale + start_x\n",
    "        py2 = ps[3][1] * scale + start_y\n",
    "        px3 = ps[2][0] * scale + start_x\n",
    "        py3 = ps[2][1] * scale + start_y\n",
    "\n",
    "    px1 = min(max(px1, 1), image_width - 1)\n",
    "    px2 = min(max(px2, 1), image_width - 1)\n",
    "    px3 = min(max(px3, 1), image_width - 1)\n",
    "    px4 = min(max(px4, 1), image_width - 1)\n",
    "    py1 = min(max(py1, 1), image_height - 1)\n",
    "    py2 = min(max(py2, 1), image_height - 1)\n",
    "    py3 = min(max(py3, 1), image_height - 1)\n",
    "    py4 = min(max(py4, 1), image_height - 1)\n",
    "    return [px1, py1, px2, py2, px3, py3, px4, py4]\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, color=None):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = (\n",
    "            color\n",
    "            if color is not None\n",
    "            else np.array([30 / 255, 144 / 255, 255 / 255, 0.5])\n",
    "        )\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_masks(masks, filename, image):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(image)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask[0].astype(np.uint8)\n",
    "        # contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # for cont in contours:\n",
    "        #     epsilon = 0.002 * cv2.arcLength(cont, True)\n",
    "        #     approx = cv2.approxPolyDP(cont, epsilon, True)\n",
    "        #     pts = approx.reshape((-1, 2))\n",
    "        #     if pts.shape[0] < 4:\n",
    "        #         continue\n",
    "        #     pts = pts.astype(np.int32)\n",
    "        #     mask = cv2.fillPoly(np.zeros(mask.shape), [pts], 1)\n",
    "        show_mask(mask, plt.gca(), random_color=True)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filename, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    # input: List[str] = field(metadata={\"help\": \"Path to the input image\"})\n",
    "    output: str = field(default=\"./demo\", metadata={\"help\": \"A file or directory to save output visualizations.\"})\n",
    "    model_type: str = field(default=\"vit_l\", metadata={\"help\": \"The type of model to load, in ['vit_h', 'vit_l', 'vit_b']\"})\n",
    "    checkpoint: str = field(metadata={\"help\": \"The path to the SAM checkpoint to use for mask generation.\"})\n",
    "    device: str = field(default=\"cuda\", metadata={\"help\": \"The device to run generation on.\"})\n",
    "    hier_det: bool = field(default=False, metadata={\"help\": \"If False, only text stroke segmentation.\"})\n",
    "    input_size: List[int] = field(default_factory=lambda: [1024, 1024], metadata={\"help\": \"The input image size.\"})\n",
    "    patch_mode: bool = field(default=False, metadata={\"help\": \"self-prompting\"})\n",
    "    attn_layers: int = field(default=1, metadata={\"help\": \"The number of image to token cross attention layers in model_aligner\"})\n",
    "    prompt_len: int = field(default=12, metadata={\"help\": \"The number of prompt token\"})\n",
    "    zero_shot: bool = field(default=False, metadata={\"help\": \"If True, use zero-shot setting.\"})\n",
    "    vis: bool = field(default=True, metadata={\"help\": \"If True, save visualization.\"})\n",
    "\n",
    "args = Args(checkpoint=\"pretrained_checkpoint/sam_vit_l.pth\", output=\"./demo\", hier_det=True,\n",
    "            input_size=[512, 512], patch_mode=True, attn_layers=1, prompt_len=12, model_type=\"vit_h\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from hi_sam.models.auto_mask_generator import AutoMaskGenerator\n",
    "\n",
    "\n",
    "model_types = ['vit_h', 'vit_l', 'vit_b']\n",
    "\n",
    "\n",
    "model = model_registry[model_types[0]](pretrained=True)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "model.eval()\n",
    "model.to(args.device)\n",
    "print(\"Loaded model\")\n",
    "amg = AutoMaskGenerator(model)\n",
    "\n",
    "if args.dataset == 'totaltext':\n",
    "    if args.zero_shot:\n",
    "        fg_points_num = 50  # assemble text kernel\n",
    "        score_thresh = 0.3\n",
    "        unclip_ratio = 1.5\n",
    "    else:\n",
    "        fg_points_num = 500\n",
    "        score_thresh = 0.95\n",
    "elif args.dataset == 'ctw1500':\n",
    "    if args.zero_shot:\n",
    "        fg_points_num = 100\n",
    "        score_thresh = 0.6\n",
    "    else:\n",
    "        fg_points_num = 300\n",
    "        score_thresh = 0.7\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "# if os.path.isdir(args.input[0]):\n",
    "#     args.input = [os.path.join(args.input[0], fname) for fname in os.listdir(args.input[0])]\n",
    "# elif len(args.input) == 1:\n",
    "#     args.input = glob.glob(os.path.expanduser(args.input[0]))\n",
    "#     assert args.input, \"The input path(s) was not found\"\n",
    "for i in tqdm(range(500)):\n",
    "    hf_dataset_base_url = \"https://huggingface.co/datasets/GoGiants1/TMDBEval500/resolve/main/TMDBEval500/images/\"\n",
    "    url = hf_dataset_base_url + f\"{i}.jpg\"\n",
    "\n",
    "    if os.path.isdir(args.output):\n",
    "        assert os.path.isdir(args.output), args.output\n",
    "        img_name = f\"{i}.png\"\n",
    "        out_filename = os.path.join(args.output, img_name)\n",
    "    else:\n",
    "        assert len(args.input) == 1\n",
    "        out_filename = args.output\n",
    "\n",
    "    image = load_image(url)\n",
    "    img_h, img_w = image.shape[:2]\n",
    "\n",
    "    image_arr = np.asarray(image)\n",
    "\n",
    "    amg.set_image(image_arr)\n",
    "    masks, scores = amg.predict_text_detection(\n",
    "        from_low_res=False,\n",
    "        fg_points_num=fg_points_num,\n",
    "        batch_points_num=min(fg_points_num, 100),\n",
    "        score_thresh=score_thresh,\n",
    "        nms_thresh=score_thresh,\n",
    "        zero_shot=args.zero_shot,\n",
    "        dataset=args.dataset\n",
    "    )\n",
    "\n",
    "    if masks is not None:\n",
    "        print('Inference done. Start plotting masks.')\n",
    "        show_masks(masks, out_filename, image)\n",
    "    else:\n",
    "        print('no prediction')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
