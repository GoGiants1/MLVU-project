{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Adapter + Attn Mask -> visualization (Attention map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 15:24:49.928098: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-26 15:24:49.971525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-26 15:24:49.971566: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-26 15:24:49.971592: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-26 15:24:49.980421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/s2/mlvu25/anaconda3/envs/tf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cbd5ea50934c2cb3d40068080a7d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading text_segmenter.pth...\n",
      "Freeze image encoder.\n",
      "<All keys matched successfully>\n",
      "Freeze image encoder.\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2/mlvu25/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/s2/mlvu25/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape:  (512, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b47bcb932847398af549013b280902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from text_diffuser.generate_mask_only import gen_bbox_mask_and_text_stroke_mask\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from text_diffuser.pipeline_text_diffuser_sd15 import StableDiffusionPipeline\n",
    "from text_diffuser.t_diffusers.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from hi_sam.text_segmentation import make_text_segmentation_args\n",
    "\n",
    "\n",
    "td_ckpt = \"GoGiants1/td-unet15\"\n",
    "\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    td_ckpt,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "pipe.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=[\n",
    "        \"models\",\n",
    "    ],\n",
    "    weight_name=[\n",
    "        \"ip-adapter_sd15.safetensors\",\n",
    "    ],\n",
    ")\n",
    "pipe.set_ip_adapter_scale(1.0)\n",
    "\n",
    "\n",
    "input_image = Image.open(\"text_diffuser/assets/test01.jpeg\").convert(\"RGB\").resize((512,512))\n",
    "\n",
    "\n",
    "\"\"\" Change the text in the original image by coordinates and  \"\"\"\n",
    "\n",
    "sample_text=\"Doge Coin\"\n",
    "# for original_input.jpeg. 110, 500에서 가장 가까운 mask의 글자를 바꾼다.\n",
    "coordinates=[[256, 256]] \n",
    "\n",
    "arg_textseg = make_text_segmentation_args(\n",
    "    model_type='vit_l',\n",
    "    checkpoint_path='sam_tss_l_hiertext.pth',\n",
    "    input_size=input_image.size,\n",
    "    hier_det=False,\n",
    ")\n",
    "\n",
    "arg_maskgen = make_text_segmentation_args(\n",
    "    model_type='vit_h',\n",
    "    checkpoint_path='word_detection_totaltext.pth',\n",
    "    input_size=input_image.size,\n",
    "    hier_det=True,\n",
    ")\n",
    "\n",
    "out, tss = gen_bbox_mask_and_text_stroke_mask(input_image, sample_text=sample_text, choice_list=coordinates, arg_textseg=arg_textseg, arg_maskgen=arg_maskgen)\n",
    "\n",
    "# out.save(f\"./assets/mask_1_out.png\") # for debugging\n",
    "text_mask_image = cv2.cvtColor(np.array(out), cv2.COLOR_RGB2BGR)\n",
    "tss_image = Image.fromarray(tss)\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "pipe.to(\"cuda\")\n",
    "output = pipe(\n",
    "    prompt=\"a man, poster\",\n",
    "    input_image=input_image,\n",
    "    text_mask_image=text_mask_image,\n",
    "    ip_adapter_image=input_image,\n",
    "    text_stroke_mask=tss,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    guidance_scale=7,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstyle_transfer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize_attention_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mip_adapter_attn_map_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attnmaps2images, get_net_attn_map\n\u001b[0;32m----> 3\u001b[0m attn_maps \u001b[38;5;241m=\u001b[39m \u001b[43mget_net_attn_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_maps\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m attn_hot \u001b[38;5;241m=\u001b[39m attnmaps2images(attn_maps)\n",
      "File \u001b[0;32m~/MLVU-project/style_transfer/visualize_attention_src/ip_adapter_attn_map_utils.py:73\u001b[0m, in \u001b[0;36mget_net_attn_map\u001b[0;34m(image_size, batch_size, instance_or_negative, detach, target_processor)\u001b[0m\n\u001b[1;32m     70\u001b[0m     attn_map \u001b[38;5;241m=\u001b[39m upscale(attn_map, image_size)\n\u001b[1;32m     71\u001b[0m     net_attn_maps\u001b[38;5;241m.\u001b[39mappend(attn_map)\n\u001b[0;32m---> 73\u001b[0m net_attn_maps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_attn_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net_attn_maps\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "from style_transfer.visualize_attention_src.ip_adapter_attn_map_utils import attnmaps2images, get_net_attn_map\n",
    "\n",
    "attn_maps = get_net_attn_map((512, 512))\n",
    "print(attn_maps.shape)\n",
    "attn_hot = attnmaps2images(attn_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "ip_adapter_image = cv2.imread(\"./assets/test01.jpeg\")\n",
    "display_images = [cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)] + attn_hot + [output]\n",
    "fig, axes = plt.subplots(1, len(display_images), figsize=(12, 4))\n",
    "for axe, image in zip(axes, display_images):\n",
    "    axe.imshow(image, cmap='gray')\n",
    "    axe.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
